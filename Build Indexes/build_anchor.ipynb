{"cells":[{"cell_type":"code","execution_count":33,"metadata":{"id":"naCwKZQ0w2-z","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-f7a7  GCE       4                                       RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","metadata":{"id":"QJTqwKoaw7Yk"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"KDIsxbFew6B9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"Lisdm_pyw6bF"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","import numpy as np\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n","\n","from builtins import *"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ao9Gm5n6w6tx"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan  8 19:23 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"kkiesBTCw7Ek"},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"C-82-N3Pxe9n"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-f7a7-m.c.assignment-3-334321.internal:34925\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f44fb68e460>"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]},{"name":"stderr","output_type":"stream","text":["22/01/08 20:07:22 WARN org.apache.spark.SparkContext: The path /home/dataproc/inverted_index_gcp.py has been added already. Overwriting of added paths is not supported in the current version.\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py\n","\n","# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","\n","from inverted_index_gcp import InvertedIndex, MultiFileReader"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","def partition_postings_and_write(postings, name):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and returns the posting locations for \n","  each bucket. Partitioning should be done through the use of `token2bucket` \n","  above. Writing to disk should use the function  `write_a_posting_list`, a \n","  static method implemented in inverted_index_colab.py under the InvertedIndex \n","  class. \n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.\n","  '''\n","  # YOUR CODE HERE\n","  save_to = postings.map(lambda x: (token2bucket_id(x[0]), x))\n","  save_to = save_to.groupByKey().mapValues(list)\n","  return save_to.map(lambda x: InvertedIndex.write_a_posting_list(x, bucket_name, name))\n","  # YOUR CODE HERE\n","    \n","\n","TUPLE_SIZE = 6       \n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","from contextlib import closing\n","\n","def read_posting_list(inverted, w):\n","  with closing(MultiFileReader(bucket_name)) as reader:\n","    locs = inverted.posting_locs[w]\n","    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n","    posting_list = []\n","    for i in range(inverted.df[w]):\n","      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","      posting_list.append((doc_id, tf))\n","#     print(posting_list)\n","    return posting_list"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def word_count(tokens, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    tokens: list\n","      list of tokens\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  dictionary = {}\n","  for token in tokens:\n","    if token not in dictionary:\n","      dictionary[token] = [id, 1]\n","    else:\n","      dictionary[token][1] += 1\n","  return [(doc_id, (dictionary[doc_id][0], dictionary[doc_id][1])) for doc_id in dictionary]\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda x: x[0])\n","  # YOUR CODE HERE\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.map(lambda x: (x[0], len(x[1])))\n","  # YOUR CODE HERE\n","\n","def f_sum(x):\n","  c=0\n","  for i in x:\n","    c += i\n","  return c\n","  \n","def calculate_term_total(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE\n","  return postings.mapValues(lambda x: [y[1] for y in x]).mapValues(f_sum)\n","  # YOUR CODE HERE"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def build_inverted_index(key_value_pairs, name):\n","  word_counts = key_value_pairs.flatMap(lambda x: word_count(tokenize(x[0]), x[1]))\n","  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","#   postings = postings.filter(lambda x: len(x[1])>50)\n","  w2df = calculate_df(postings)\n","  w2df_dict = w2df.collectAsMap()\n","  _ = partition_postings_and_write(postings, name).collect() #TODO: _ was \"posting_locs_list\" before\n","\n","  super_posting_locs = defaultdict(list)\n","  for blob in client.list_blobs(bucket_name, prefix=f'postings_gcp/{name}_index/'):\n","    if not blob.name.endswith(\"pickle\"):\n","      continue\n","    with blob.open(\"rb\") as f:\n","      posting_locs = pickle.load(f)\n","      for k, v in posting_locs.items():\n","        super_posting_locs[k].extend(v)\n","  \n","  # build DL\n","  DL = key_value_pairs.map(lambda x: (x[1], len(tokenize(x[0]))))\n","  #build term_total\n","  term_total = calculate_term_total(postings).groupByKey().mapValues(f_sum)\n","\n","  # Create inverted index instance\n","  index = InvertedIndex()\n","  # Adding the posting locations dictionary to the inverted index\n","  index.posting_locs = super_posting_locs\n","  # Add the token - df dictionary to the inverted index\n","  index.df = w2df_dict\n","  # Add the DL\n","  index.DL = dict(DL.collect())\n","  # term_total\n","  index.term_total = dict(term_total.collect())\n","  # write the global stats out\n","  index.write_index('.', name)\n","\n","  # upload to gs\n","  index_src = name + \".pkl\"\n","  index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n","  !gsutil cp $index_src $index_dst\n","  !gsutil ls -lh $index_dst\n","\n","  return index"]},{"cell_type":"markdown","metadata":{"id":"jdHU1z6Kxg2j"},"source":["# configs"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"9ONrFnUKxgiV"},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'fadlonbucket' #TODO: change the bucket name to our GCP bucket\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name, prefix='postings_gcp')\n"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]  # TODO: calculate the corups stop words\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","stemmer = PorterStemmer()\n","\n","def tokenize(text, use_stemmer=False):\n","  \"\"\"\n","    This function aims in tokenize a text into a list of tokens.\n","    Moreover:\n","    * filter stopwords.\n","    * change all to lowwer case.\n","    * use stemmer\n","    \n","    Parameters:\n","    -----------\n","    text: string , represting the text to tokenize.    \n","    \n","    Returns:\n","    -----------\n","    list of tokens (e.g., list of tokens).\n","    \"\"\"\n","  clean_text = []\n","\n","  text = text.lower()\n","  tokens = [token.group() for token in RE_WORD.finditer(text)]\n","  for token in tokens:\n","    if token not in all_stopwords:\n","      if use_stemmer:\n","         token = stemmer.stem(token)\n","      clean_text.append(token)\n","  return clean_text"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["import pickle\n","\n","def open_gcp(file_name):\n","    client = storage.Client(file_name)\n","    bucket = client.bucket(bucket_name)\n","    blob = bucket.get_blob('postings_gcp/' + file_name)\n","    return blob.open('rb')\n","\n","def read_pickle(file_name):\n","    stream = open_gcp(file_name+\".pkl\")\n","    pick = pickle.load(stream)\n","    stream.close()\n","    return pick"]},{"cell_type":"markdown","metadata":{"id":"FuUS7o3SxuE-"},"source":["# Building Inverted Index"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"v9E7RFdrxuiZ"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 16:========================================>             (94 + 16) / 124]\r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_12742/3966284474.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mparquetFile\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfull_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# Count number of wiki pages\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mparquetFile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mcount\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    662\u001B[0m         \u001B[0;36m2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    663\u001B[0m         \"\"\"\n\u001B[0;32m--> 664\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    665\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    666\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1301\u001B[0m             \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEND_COMMAND_PART\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1303\u001B[0;31m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1031\u001B[0m         \u001B[0mconnection\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_connection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1032\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1033\u001B[0;31m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1034\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mbinary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1035\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_connection_guard\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconnection\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36msend_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m   1198\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1199\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1200\u001B[0;31m             \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msmart_decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1201\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Answer received: {0}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1202\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mRETURN_MESSAGE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/opt/conda/miniconda3/lib/python3.8/socket.py\u001B[0m in \u001B[0;36mreadinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    667\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    668\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 669\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_into\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    670\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    671\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_timeout_occurred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mKeyboardInterrupt\u001B[0m: "]}],"source":["full_path = \"gs://wikidata_preprocessed/*\" #TODO: drop wikidumps in url\n","parquetFile = spark.read.parquet(full_path)\n","# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GMsoCkhyiTP"},"outputs":[],"source":["doc_text_pairs = parquetFile.select(\"anchor_text\").rdd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def to_pairs(row):\n","    pairs = []\n","    for doc_id, anchor_text in row[0]:\n","        pairs.append((doc_id, anchor_text))\n","    return pairs\n","\n","all_anchor_text = doc_text_pairs.flatMap(to_pairs).groupByKey().mapValues(list).map(lambda x: (\" \".join(x[1]), x[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inverted_anchor = build_inverted_index(all_anchor_text, \"anchor_fix\")\n","print(\"Done\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"project_GCP.ipynb","provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":1}