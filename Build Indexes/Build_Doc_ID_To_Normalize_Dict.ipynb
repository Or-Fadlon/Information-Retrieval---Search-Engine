{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naCwKZQ0w2-z",
        "scrolled": true,
        "outputId": "2fbdd95a-db7f-4ceb-aeac-0088fdc0589c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n",
            "cluster-220a  GCE       4                                       RUNNING  us-central1-a\r\n"
          ]
        }
      ],
      "source": [
        "# if the following command generates an error, you probably didn't enable \n",
        "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
        "# under Manage Security â†’ Project Access when setting up the cluster\n",
        "!gcloud dataproc clusters list --region us-central1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJTqwKoaw7Yk"
      },
      "source": [
        "# Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDIsxbFew6B9",
        "outputId": "c4e7c2d2-2de9-4404-fa8f-ee4058f5419b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lisdm_pyw6bF",
        "outputId": "b05d2d26-4366-4b43-ef3a-938b78466d02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from builtins import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao9Gm5n6w6tx",
        "outputId": "838e1aac-5533-4277-d001-d24f848a6468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 247882 Jan  8 12:19 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
          ]
        }
      ],
      "source": [
        "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
        "!ls -l /usr/lib/spark/jars/graph*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkiesBTCw7Ek"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-82-N3Pxe9n",
        "outputId": "72bd32b0-2cba-4d12-b690-44e1d7d994a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cluster-220a-m.c.assignment-3-334321.internal:42101\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f67360f7400>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdHU1z6Kxg2j"
      },
      "source": [
        "# configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ONrFnUKxgiV"
      },
      "outputs": [],
      "source": [
        "# TODO: Put your bucket name below and make sure you can access it without an error\n",
        "bucket_name = 'fadlonbucket'\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name, prefix='postings_gcp')\n",
        "# # print all the blobs in the bucket\n",
        "# for b in blobs:\n",
        "#     print(b.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2pDZ55Zx8B3"
      },
      "outputs": [],
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\", \"became\"]  # TODO: calculate the corups stop words\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenize(text, use_stemmer=False):\n",
        "  \"\"\"\n",
        "    This function aims in tokenize a text into a list of tokens.\n",
        "    Moreover:\n",
        "    * filter stopwords.\n",
        "    * change all to lowwer case.\n",
        "    * use stemmer\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: string , represting the text to tokenize.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    list of tokens (e.g., list of tokens).\n",
        "    \"\"\"\n",
        "  clean_text = []\n",
        "\n",
        "  text = text.lower()\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text)]\n",
        "  for token in tokens:\n",
        "    if token not in all_stopwords:\n",
        "      if use_stemmer:\n",
        "         token = stemmer.stem(token)\n",
        "      clean_text.append(token)\n",
        "  return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhkgRWDcV9nb"
      },
      "outputs": [],
      "source": [
        "def open_gcp(file_name):\n",
        "  \"\"\"\n",
        "    get a read bit stream to a gcp blob in the bucket.\n",
        "    start with the prefix: 'postings_gcp/'\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_name: string , a path to the file.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    stream to read from\n",
        "    \"\"\"\n",
        "    client = storage.Client(file_name)\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.get_blob('postings_gcp/' + file_name)\n",
        "    return blob.open('rb')\n",
        "\n",
        "def read_pickle(file_name):\n",
        "  \"\"\"\n",
        "    read a pickle file from the bucket.\n",
        "    from 'postings_gcp/'\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_name: string , name of the file.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    object that readed from pickle\n",
        "    \"\"\"\n",
        "    stream = open_gcp(file_name+\".pkl\")\n",
        "    pick = pickle.load(stream)\n",
        "    stream.close()\n",
        "    return pick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuUS7o3SxuE-"
      },
      "source": [
        "# Building Doc-id to Normalize Factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9E7RFdrxuiZ",
        "outputId": "f11445da-cea7-4f01-90bc-8e53d965b113"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6348910"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#path to the full wiki data\n",
        "full_path = \"gs://wikidata_preprocessed/*\"\n",
        "#load to spark\n",
        "parquetFile = spark.read.parquet(full_path)\n",
        "# Count number of wiki pages\n",
        "parquetFile.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rBAWAdEV9ne"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def build_norm_doc_i(tok_text):\n",
        "    temp_dict = {}\n",
        "    for term in list(tok_text):\n",
        "        if term not in temp_dict:\n",
        "            temp_dict[term] = 0\n",
        "        temp_dict[term] +=1\n",
        "    c = 0\n",
        "    for term in temp_dict:\n",
        "        c += temp_dict[term]**2\n",
        "    if c == 0:\n",
        "        return c\n",
        "    return 1/math.sqrt(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GMsoCkhyiTP"
      },
      "outputs": [],
      "source": [
        "# rdd of text and doc_id\n",
        "doc_text_pairs = parquetFile.select(\"id\", \"text\").rdd\n",
        "\n",
        "doc_text_pairs_new = doc_text_pairs.mapValues(tokenize).mapValues(build_norm_doc_i)\n",
        "\n",
        "doc_id_norm_dict= dict(doc_text_pairs_new.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save to bucket"
      ],
      "metadata": {
        "id": "bNVAdAkmWizB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i47Hf5J6V9nh"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "name = \"doc_id_norm_dict\"\n",
        "with open(f\"{name}.pkl\", \"wb\") as f:\n",
        "    pickle.dump(doc_id_norm_dict, f)\n",
        "    \n",
        "client = storage.Client()\n",
        "bucket = client.bucket(bucket_name)\n",
        "blob_posting_locs = bucket.blob(f\"postings_gcp/{name}.pkl\")\n",
        "blob_posting_locs.upload_from_filename(f\"{name}.pkl\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Build_Doc_ID_To_Normalize_Dict.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}