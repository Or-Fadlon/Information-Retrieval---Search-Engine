{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FrBdFNYgiyab"},"outputs":[],"source":["# install ngrok to emulate public IP / address\n","!wget -N https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\n","!unzip -u ngrok-stable-linux-amd64.zip"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -q gsutil"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -q google-cloud-storage==1.43.0\n","from google.cloud import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xA-_dKEZbsXW"},"outputs":[],"source":["# TODO: sign up for an ngrok account\n","# then put your ngrok token below, uncomment, and execute\n","!./ngrok authtoken 23Kbw3CcDDS20BAb1UEm7UknyJn_5gwinxnqscxNH9qgbFdHx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXn5PlyGORQr"},"outputs":[],"source":["# install a ngrok python package and a version of flask that works with it in \n","# colab\n","!pip -q install flask-ngrok\n","!pip -q install flask==0.12.2\n","# !pip -q install flask_restful"]},{"cell_type":"markdown","metadata":{"id":"6dW0y91OVu5J"},"source":["# Run the app"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTGXXYEXV5l8"},"outputs":[],"source":["# adding our python module to the cluster\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py\n","from inverted_index_gcp import *\n","\n","# uncomment the code below and execute to reload the module when you make \n","# changes to search_frontend.py (after you upload again).\n","# import importlib\n","# importlib.reload(se)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import pyspark\n","from pathlib import Path\n","from contextlib import closing\n","# import pandas as pd\n","# import numpy as np\n","import sys\n","from collections import Counter, OrderedDict\n","import itertools\n","from itertools import islice, count, groupby\n","import os\n","import re\n","from operator import itemgetter\n","import functools\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from builtins import *"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bucket_name = 'fadlonbucket'\n","\n","import pickle\n","def open_gcp(file_name):\n","    client = storage.Client(file_name)\n","    bucket = client.bucket(bucket_name)\n","    blob = bucket.get_blob('postings_gcp/' + file_name)\n","    return blob.open('rb')\n","\n","def read_pickle(file_name):\n","    stream = open_gcp(file_name+\".pkl\")\n","    pick = pickle.load(stream)\n","    stream.close()\n","    print(file_name)\n","    return pick\n","\n","def get_title_by_doc_id(doc_id):\n","    try:\n","        return doc_id_to_title_dic[doc_id]\n","    except:\n","        return \"Invalid Title!\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Start loading...\")\n","inverted_title = read_pickle(\"title2\")\n","inverted_anchor = read_pickle(\"anchor_fix\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["page_rank_dict = read_pickle(\"page_rank_dict\")\n","page_view_dict = read_pickle(\"page_view\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inverted_body = read_pickle(\"text2\")\n","doc_id_norm_dict = read_pickle(\"doc_id_norm_dict\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["doc_id_to_title_dic = read_pickle(\"doc_id_to_title_dict\")\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TUPLE_SIZE = 6       \n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n","from contextlib import closing\n","\n","def read_posting_list(inverted, w):\n","  with closing(MultiFileReader(bucket_name)) as reader:\n","    locs = inverted.posting_locs[w]\n","    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n","    posting_list = []\n","    for i in range(inverted.df[w]):\n","      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n","      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n","      posting_list.append((doc_id, tf))\n","    return posting_list\n","\n","\n","nltk.download('stopwords')\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]  # TODO: calculate the corups stop words words\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","stemmer = PorterStemmer()\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","def tokenize(text, stem=False):\n","  \"\"\"\n","    This function aims in tokenize a text into a list of tokens.\n","    Moreover:\n","    * filter stopwords.\n","    * change all to lowwer case.\n","    * use stemmer\n","    \n","    Parameters:\n","    -----------\n","    text: string , represting the text to tokenize.    \n","    \n","    Returns:\n","    -----------\n","    list of tokens (e.g., list of tokens).\n","    \"\"\"\n","  clean_text = []\n","\n","  text = text.lower()\n","  tokens = [token.group() for token in RE_WORD.finditer(text)]\n","  for token in tokens:\n","    if token not in all_stopwords:\n","      if stem:\n","        token = stemmer.stem(token)\n","      clean_text.append(token)\n","  return clean_text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","from itertools import chain\n","import time\n","\n","def get_posting_gen(index, query):\n","    \"\"\"\n","    This function returning the generator working with posting list.\n","    \n","    Parameters:\n","    ----------\n","    index: inverted index    \n","    \"\"\"\n","    words = []\n","    pls = []\n","    for term in query:\n","        try:\n","            pls = read_posting_list(index, term)\n","        except:\n","            pls = []\n","        yield term, pls\n","\n","# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.\n","class BM25_from_index:\n","    \"\"\"\n","    Best Match 25.    \n","    ----------\n","    k1 : float, default 1.5\n","\n","    b : float, default 0.75\n","\n","    index: inverted index\n","    \"\"\"\n","\n","    def __init__(self,index,k1=1.5, b=0.75):\n","        self.b = b\n","        self.k1 = k1\n","        self.index = index\n","        self.N = len(index.DL)\n","        self.AVGDL = sum(index.DL.values())/self.N \n","\n","    def calc_idf(self,list_of_tokens):\n","        \"\"\"\n","        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        \n","        Returns:\n","        -----------\n","        idf: dictionary of idf scores. As follows: \n","                                                    key: term\n","                                                    value: bm25 idf score\n","        \"\"\"        \n","        idf = {}        \n","        for term in list_of_tokens:            \n","            if term in self.index.df.keys():\n","                n_ti = self.index.df[term]\n","                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n","            else:\n","                pass                             \n","        return idf\n","        \n","\n","    def search(self, query,N=100):\n","        \"\"\"\n","        This function calculate the bm25 score for given query and document.\n","        We need to check only documents which are 'candidates' for a given query. \n","        This function return a dictionary of scores as the following:\n","                                                                    key: query_id\n","                                                                    value: a ranked list of pairs (doc_id, score) in the length of N.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        doc_id: integer, document id.\n","        \n","        Returns:\n","        -----------\n","        score: float, bm25 score.\n","        \"\"\"\n","        # YOUR CODE HERE\n","        query = tokenize(query)\n","        candidate = set([])\n","        for w, pls in get_posting_gen(self.index, query):\n","            for doc_id, frq in pls:\n","                candidate.add(doc_id)\n","        self.idf = self.calc_idf(query)\n","        results = self._score(query, candidate)\n","        query_top_n = sorted(results, key = lambda x: x[1], reverse=True)[:N]\n","        return query_top_n\n","        # YOUR CODE HERE\n","\n","    def _score(self, query, candidates):\n","        \"\"\"\n","        This function calculate the bm25 score for given query and document.\n","        \n","        Parameters:\n","        -----------\n","        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n","        doc_id: integer, document id.\n","        \n","        Returns:\n","        -----------\n","        score: float, bm25 score.\n","        \"\"\"\n","        score_ret = {}\n","        for w, pls in get_posting_gen(self.index, query):\n","            term_frequencies = dict(pls)\n","            for doc_id in candidates:\n","                score = 0.0\n","                doc_len = self.index.DL[doc_id]\n","                if doc_id in term_frequencies:\n","                    freq = term_frequencies[doc_id]\n","                    numerator = self.idf[w] * freq * (self.k1 + 1)\n","                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n","                    score += (numerator / denominator)\n","                score_ret[doc_id] = score_ret.get(doc_id, 0) + score\n","        return list(score_ret.items())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def merge_results(title_scores,body_scores,title_weight=0.5,text_weight=0.5,N = 100):    \n","    \"\"\"\n","    This function merge and sort documents retrieved by its weighte score (e.g., title and body). \n","\n","    Parameters:\n","    -----------\n","    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","                \n","    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows: \n","                                                                            key: query_id\n","                                                                            value: list of pairs in the following format:(doc_id,score)\n","    title_weight: float, for weigted average utilizing title and body scores\n","    text_weight: float, for weigted average utilizing title and body scores\n","    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function. \n","    \n","    Returns:\n","    -----------\n","    dictionary of querires and topN pairs as follows:\n","                                                        key: query_id\n","                                                        value: list of pairs in the following format:(doc_id,score). \n","    \"\"\"\n","    temp_dict = {}\n","    for doc_id, score in title_scores:\n","        temp_dict[doc_id] = title_weight * score\n","    for doc_id, score in body_scores:\n","        if temp_dict.get(doc_id) is None:\n","            temp_dict[doc_id] = text_weight * score\n","        else:\n","            temp_dict[doc_id] += text_weight * score\n","    return temp_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import threading\n","import queue\n","\n","w_title = 0.25\n","w_text = 1 - w_title\n","\n","bm_weight = 0.5\n","page_view_weight = 0.25\n","page_rank_weight = 0.25\n","\n","def thread_bm25(index, query, queue):\n","    bm25_t = BM25_from_index(index)\n","    bm25_queries_score_train_t = bm25_t.search(query, N=50)\n","    queue.put(bm25_queries_score_train_t)\n","\n","def get_bm25(query):\n","    queue_title = queue.Queue()\n","    queue_body = queue.Queue()\n","    t_title = threading.Thread(target=thread_bm25, args=(inverted_title, query, queue_title))\n","    t_body = threading.Thread(target=thread_bm25, args=(inverted_body, query, queue_body))\n","    t_title.start()\n","    t_body.start()\n","    t_title.join()\n","    t_body.join()\n","    bm25_queries_score_train_title = queue_title.get()\n","    bm25_queries_score_train_body = queue_body.get()\n","    BM25_score = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body,w_title,w_text)\n","    return BM25_score\n","\n","def add_page_rank_and_view(dic):\n","    max_bm25 = 0\n","    max_page_rank = 0\n","    max_page_view = 0\n","    for key in dic:\n","        bm = dic[key]\n","        page_rank = page_rank_dict[key][0]\n","        page_view = page_view_dict[key]\n","        if bm > max_bm25:\n","            max_bm25 = bm\n","        if page_rank > max_page_rank:\n","            max_page_rank = page_rank\n","        if page_view > max_page_view:\n","            max_page_view = page_view\n","            \n","    for key in dic:\n","        bm = dic[key]\n","        page_rank = page_rank_dict[key][0]\n","        page_view = page_view_dict[key]\n","        dic[key] = round((bm * bm_weight / max_bm25) + (page_rank * page_rank_weight / max_page_rank) + (page_view * page_view_weight / max_page_view), 5)\n","    return dic"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def search_procedure(query):\n","    try:\n","        BM25 = get_bm25(query)\n","        calculated = add_page_rank_and_view(BM25)\n","        sor = list(sorted([(doc_id, calculated[doc_id]) for doc_id in calculated], key = lambda x: x[1], reverse=True)[:100])\n","        res = map(lambda x: (x[0], get_title_by_doc_id(x[0])), sor)\n","        return list(res)\n","    except Exception as e:\n","        print(f'Error - {e}')\n","        return []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from builtins import *\n","import math\n","  \n","def get_top_n(sim_dict,N=3):\n","    \"\"\" \n","    Sort and return the highest N documents according to the cosine similarity score.\n","    Generate a dictionary of cosine similarity scores \n","   \n","    Parameters:\n","    -----------\n","    sim_dict: a dictionary of similarity score as follows:\n","                                                                key: document id (e.g., doc_id)\n","                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n","\n","    N: Integer (how many documents to retrieve). By default N = 3\n","    \n","    Returns:\n","    -----------\n","    a ranked list of pairs (doc_id, score) in the length of N.\n","    \"\"\"\n","    lst = [(doc_id,round(score,5)) for doc_id, score in sim_dict.items()]\n","    srot = sorted(lst, key = lambda x: x[1],reverse=True)\n","    return srot[:N]\n","\n","def get_posting_gen(index, query):\n","    \"\"\"\n","    This function returning the generator working with posting list.\n","    \n","    Parameters:\n","    ----------\n","    index: inverted index    \n","    \"\"\"\n","    words = []\n","    pls = []\n","    for term in query:\n","        try:\n","            pls = read_posting_list(index, term)\n","        except:\n","            pls = []\n","        yield term, pls\n","\n","def norm_query(query_counter):\n","    c = 0\n","    for key in query_counter:\n","        c+=query_counter[key]**2\n","    return (1/math.sqrt(c))\n","\n","def similarity(query_to_search,index,N=3):\n","    new_query = tokenize(query_to_search)\n","    query_counter={}\n","    for i in new_query:\n","        if i not in query_counter:\n","            query_counter[i] = 0\n","        query_counter[i]+=1\n","    generator = get_posting_gen(index, list(set(new_query)))\n","    sim_dict = {}\n","    for word,pls in generator:\n","        for doc_id,weight in pls:\n","            if doc_id not in sim_dict:\n","                sim_dict[doc_id] = 0\n","            sim_dict[doc_id] = sim_dict[doc_id] +query_counter[word]*weight\n","    for key in sim_dict:\n","        sim_dict[key] = sim_dict[key]*(norm_query(query_counter))*(doc_id_norm_dict[doc_id])\n","    return get_top_n(sim_dict,N)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def search_body_procedure(query):\n","    try:\n","        cos = similarity(query,inverted_body,N = 100)\n","        cos = map(lambda x: (x[0], get_title_by_doc_id(x[0])), cos)\n","        return list(cos)\n","    except Exception as e:\n","        print(f'Error!!! - {e}')\n","        return []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def search_title_procedure(query):\n","    query = tokenize(query)\n","    results = []\n","    for term in query:\n","        try:\n","            results.append(read_posting_list(inverted_title, term))\n","        except:\n","            print(\"Term not in inverted_title: \" + term)\n","            pass\n","    \n","    if len(results) != 0:\n","        results = functools.reduce(lambda a, b: a+b, results)\n","        results = map(lambda x: x[0], results)\n","        counter = Counter()\n","        counter.update(results)\n","        results = map(lambda x: (x[0], get_title_by_doc_id(x[0])), counter.most_common())\n","\n","    return list(results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def search_anchor_procedure(query):\n","    query = tokenize(query)\n","    results = []\n","    for term in query:\n","        try:\n","            results.append(read_posting_list(inverted_anchor, term))\n","        except:\n","            print(\"Term not in inverted_anchor: \" + term)\n","            pass\n","        \n","    if len(results) != 0:\n","        results = functools.reduce(lambda a, b: a+b, results)\n","        results = map(lambda x: x[0], results)\n","        counter = Counter()\n","        counter.update(results)\n","        results = map(lambda x: (x[0], get_title_by_doc_id(x[0])), counter.most_common())\n","    return list(results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def page_rank_procedure(wiki_ids):\n","    res = []\n","    for doc_id in wiki_ids:\n","        try:\n","            res.extend(page_rank_dict[doc_id])\n","        except:\n","            print(\"doc_id not in page_rank_dict: \" + str(doc_id))\n","            res.append(0)\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def page_view_procedure(lst):\n","    res = []\n","    for doc_id in lst:\n","        try:\n","            res.append(page_view_dict[doc_id])\n","        except:\n","            print(\"doc_id not in page_view: \" + str(doc_id))\n","            res.append(0)\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfzN9kkxhs6n"},"outputs":[],"source":["from flask import Flask, request, jsonify\n","\n","class MyFlaskApp(Flask):\n","    def run(self, host=None, port=None, debug=None, **options):\n","        super(MyFlaskApp, self).run(host=host, port=port, debug=debug, **options)\n","\n","app = MyFlaskApp(__name__)\n","app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False\n","\n","\n","@app.route(\"/search\")\n","def search():\n","    ''' Returns up to a 100 of your best search results for the query. This is \n","        the place to put forward your best search engine, and you are free to\n","        implement the retrieval whoever you'd like within the bound of the \n","        project requirements (efficiency, quality, etc.). That means it is up to\n","        you to decide on whether to use stemming, remove stopwords, use \n","        PageRank, query expansion, etc.\n","\n","        To issue a query navigate to a URL like:\n","         http://YOUR_SERVER_DOMAIN/search?query=hello+world\n","        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of up to 100 search results, ordered from best to worst where each \n","        element is a tuple (wiki_id, title).\n","    '''\n","    res = []\n","    query = request.args.get('query', '')\n","    if len(query) == 0:\n","      return jsonify(res)\n","    # BEGIN SOLUTION\n","    res = search_procedure(query)\n","    # END SOLUTION\n","    return jsonify(res)\n","\n","@app.route(\"/search_body\")\n","def search_body():\n","    ''' Returns up to a 100 search results for the query using TFIDF AND COSINE\n","        SIMILARITY OF THE BODY OF ARTICLES ONLY. DO NOT use stemming. DO USE the \n","        staff-provided tokenizer from Assignment 3 (GCP part) to do the \n","        tokenization and remove stopwords. \n","\n","        To issue a query navigate to a URL like:\n","         http://YOUR_SERVER_DOMAIN/search_body?query=hello+world\n","        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of up to 100 search results, ordered from best to worst where each \n","        element is a tuple (wiki_id, title).\n","    '''\n","    res = []\n","    query = request.args.get('query', '')\n","    if len(query) == 0:\n","      return jsonify(res)\n","    # BEGIN SOLUTION\n","    res = search_body_procedure(query)\n","    # END SOLUTION\n","    return jsonify(res)\n","\n","@app.route(\"/search_title\")\n","def search_title():\n","    ''' Returns ALL (not just top 100) search results that contain A QUERY WORD \n","        IN THE TITLE of articles, ordered in descending order of the NUMBER OF \n","        QUERY WORDS that appear in the title. For example, a document with a \n","        title that matches two of the query words will be ranked before a \n","        document with a title that matches only one query term. \n","\n","        Test this by navigating to the a URL like:\n","         http://YOUR_SERVER_DOMAIN/search_title?query=hello+world\n","        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of ALL (not just top 100) search results, ordered from best to \n","        worst where each element is a tuple (wiki_id, title).\n","    '''\n","    res = []\n","    query = request.args.get('query', '')\n","    if len(query) == 0:\n","      return jsonify(res)\n","    # BEGIN SOLUTION\n","    res = search_title_procedure(query)\n","    # END SOLUTION\n","    return jsonify(res)\n","\n","@app.route(\"/search_anchor\")\n","def search_anchor():\n","    ''' Returns ALL (not just top 100) search results that contain A QUERY WORD \n","        IN THE ANCHOR TEXT of articles, ordered in descending order of the \n","        NUMBER OF QUERY WORDS that appear in anchor text linking to the page. \n","        For example, a document with a anchor text that matches two of the \n","        query words will be ranked before a document with anchor text that \n","        matches only one query term. \n","\n","        Test this by navigating to the a URL like:\n","         http://YOUR_SERVER_DOMAIN/search_anchor?query=hello+world\n","        where YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of ALL (not just top 100) search results, ordered from best to \n","        worst where each element is a tuple (wiki_id, title).\n","    '''\n","    res = []\n","    query = request.args.get('query', '')\n","    if len(query) == 0:\n","      return jsonify(res)\n","    # BEGIN SOLUTION\n","    res = search_anchor_procedure(query)\n","    # END SOLUTION\n","    return jsonify(res)\n","\n","@app.route(\"/get_pagerank\", methods=['POST'])\n","def get_pagerank():\n","    ''' Returns PageRank values for a list of provided wiki article IDs. \n","\n","        Test this by issuing a POST request to a URL like:\n","          http://YOUR_SERVER_DOMAIN/get_pagerank\n","        with a json payload of the list of article ids. In python do:\n","          import requests\n","          requests.post('http://YOUR_SERVER_DOMAIN/get_pagerank', json=[1,5,8])\n","        As before YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of floats:\n","          list of PageRank scores that correrspond to the provided article IDs.\n","    '''\n","    res = []\n","    wiki_ids = request.get_json(force=True, silent=True, cache=False)\n","    if len(wiki_ids) == 0:\n","      return jsonify(res)\n","    # BEGIN SOLUTION\n","    res =  page_rank_procedure(wiki_ids)\n","    # END SOLUTION\n","    return jsonify(res)\n","\n","@app.route(\"/get_pageview\", methods=['POST'])\n","def get_pageview():\n","    ''' Returns the number of page views that each of the provide wiki articles\n","        had in August 2021.\n","\n","        Test this by issuing a POST request to a URL like:\n","          http://YOUR_SERVER_DOMAIN/get_pageview\n","        with a json payload of the list of article ids. In python do:\n","          import requests\n","          requests.post('http://YOUR_SERVER_DOMAIN/get_pageview', json=[1,5,8])\n","        As before YOUR_SERVER_DOMAIN is something like XXXX-XX-XX-XX-XX.ngrok.io\n","        if you're using ngrok on Colab or your external IP on GCP.\n","    Returns:\n","    --------\n","        list of ints:\n","          list of page view numbers from August 2021 that correrspond to the \n","          provided list article IDs.\n","    '''\n","    res = []\n","    wiki_ids = request.get_json(force=True, silent=True, cache=False)\n","    if len(wiki_ids) == 0:\n","      return jsonify(res)\n","    # BEGIN SOLUTION\n","    res = page_view_procedure(wiki_ids)\n","    # END SOLUTION\n","    return jsonify(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5n9u9rFP_wD"},"outputs":[],"source":["from flask_ngrok import run_with_ngrok\n","from multiprocessing import Process\n","\n","run_with_ngrok(app) \n","server = Process(target=app.run)\n","server.start()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# server.terminate()\n","# server.join()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"run_frontend_in_colab.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":1}